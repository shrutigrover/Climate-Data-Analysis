{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All Features except min,max,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import timeit\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "spark = SparkSession.builder.appName(\"Project\").master(\"local[*]\").config('spark.ui.port', random.randrange(4000,5000)).getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPreProcessing(df):  \n",
    "    #casting year, month and day to integer values\n",
    "    df = df.withColumn(\"year\", df[\"year\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"mo\", df[\"mo\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"da\", df[\"da\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"temp\", df[\"temp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"slp\", df[\"slp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"stp\", df[\"stp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"wdsp\", df[\"wdsp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"mxpsd\", df[\"mxpsd\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"max\", df[\"max\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"min\", df[\"min\"].cast(FloatType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_and_set_views():\n",
    "    global stations, gsod_data_2009, gsod_data_2010, gsod_data_2011, gsod_data_2012, gsod_data_2013, gsod_data_2014, gsod_data_2015, gsod_data_2016, gsod_data_2017, gsod_data_2018, gsod_data_2019\n",
    "\n",
    "    gsod_data_2009 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2009.txt\").cache()\n",
    "#     gsod_data_2009=performPreProcessing(gsod_data_2009)\n",
    "    gsod_data_2009.createOrReplaceTempView(\"gsod_data_2009\")\n",
    "\n",
    "    gsod_data_2010 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2010.txt\").cache()\n",
    "#     gsod_data_2010 = performPreProcessing(gsod_data_2010)\n",
    "    gsod_data_2010.createOrReplaceTempView(\"gsod_data_2010\")\n",
    "\n",
    "    gsod_data_2011 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2011.txt\").cache()\n",
    "#     gsod_data_2011 =performPreProcessing(gsod_data_2011)\n",
    "    gsod_data_2011.createOrReplaceTempView(\"gsod_data_2011\")\n",
    "\n",
    "    gsod_data_2012 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2012.txt\").cache()\n",
    "#     gsod_data_2012 =performPreProcessing(gsod_data_2012)\n",
    "    gsod_data_2012.createOrReplaceTempView(\"gsod_data_2012\")\n",
    "\n",
    "    gsod_data_2013 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2013.txt\").cache()\n",
    "#     gsod_data_2013 =performPreProcessing(gsod_data_2013)\n",
    "    gsod_data_2013.createOrReplaceTempView(\"gsod_data_2013\")\n",
    "\n",
    "    gsod_data_2014 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2014.txt\").cache()\n",
    "#     gsod_data_2014 =performPreProcessing(gsod_data_2014)\n",
    "    gsod_data_2014.createOrReplaceTempView(\"gsod_data_2014\")\n",
    "\n",
    "    gsod_data_2015 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2015.txt\").cache()\n",
    "#     gsod_data_2015 =performPreProcessing(gsod_data_2015)\n",
    "    gsod_data_2015.createOrReplaceTempView(\"gsod_data_2015\")\n",
    "\n",
    "    gsod_data_2016 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2016.txt\").cache()\n",
    "#     gsod_data_2016 =performPreProcessing(gsod_data_2016)\n",
    "    gsod_data_2016.createOrReplaceTempView(\"gsod_data_2016\")\n",
    "\n",
    "    gsod_data_2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2017.txt\").cache()\n",
    "#     gsod_data_2017 =performPreProcessing(gsod_data_2017)\n",
    "    gsod_data_2017.createOrReplaceTempView(\"gsod_data_2017\")\n",
    "\n",
    "    gsod_data_2018 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2018.txt\").cache()\n",
    "#     gsod_data_2018 =performPreProcessing(gsod_data_2018)\n",
    "    gsod_data_2018.createOrReplaceTempView(\"gsod_data_2018\")\n",
    "\n",
    "    gsod_data_2019 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2019.txt\").cache()\n",
    "#     gsod_data_2019 =performPreProcessing(gsod_data_2019)\n",
    "    gsod_data_2019.createOrReplaceTempView(\"gsod_data_2019\")\n",
    "\n",
    "    stations = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/stations.txt\").cache()\n",
    "    stations.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "load_datasets_and_set_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsod_data_2009.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsod_data_2009.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=\"01\"\n",
    "country=\"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2009=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2009 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2010=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2010 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2011=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2011 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2012=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2012 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2013=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2013 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2014=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2014 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2015=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2015 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2016=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2016 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2017=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2017 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2018=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2019=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_2019_all=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn  AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_2018_all=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2009 =performPreProcessing(query_2009)\n",
    "query_2010 =performPreProcessing(query_2010)\n",
    "query_2011 =performPreProcessing(query_2011)\n",
    "query_2012 =performPreProcessing(query_2012)\n",
    "query_2013 =performPreProcessing(query_2013)\n",
    "query_2014 =performPreProcessing(query_2014)\n",
    "query_2015 =performPreProcessing(query_2015)\n",
    "query_2016 =performPreProcessing(query_2016)\n",
    "query_2017 =performPreProcessing(query_2017)\n",
    "query_2018 =performPreProcessing(query_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=query_2009.union(query_2010)\\\n",
    "#                 .union(query_2011)\\\n",
    "#                 .union(query_2012)\\\n",
    "#                 .union(query_2013)\\\n",
    "#                 .union(query_2014)\\\n",
    "#                 .union(query_2015)\\\n",
    "#                 .union(query_2016)\\\n",
    "#                 .union(query_2017)\\\n",
    "#                 .union(query_2018)\\\n",
    "#                 .union(query_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all=query_2019_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Monthly Average Temperature\n",
    "df_min=df_all.groupBy('stn').agg({'temp':'min'})\n",
    "df_max=df_all.groupBy('stn').agg({'temp':'max'})\n",
    "df_avg=df_all.groupBy('stn').agg({'temp':'avg'})\n",
    "# df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_features = [t[0] for t in df_all.dtypes if (t[1] == 'float')]\n",
    "# print(numeric_features)\n",
    "# df_all.select(numeric_features).describe().toPandas().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# numeric_data = df_all.select(numeric_features).toPandas()\n",
    "# axs = scatter_matrix(numeric_data, figsize=(8, 8))\n",
    "# n = len(numeric_data.columns)\n",
    "# for i in range(n):\n",
    "#     v = axs[i, 0]\n",
    "#     v.yaxis.label.set_rotation(0)\n",
    "#     v.yaxis.label.set_ha('right')\n",
    "#     v.set_yticks(())\n",
    "#     h = axs[n-1, i]\n",
    "#     h.xaxis.label.set_rotation(90)\n",
    "#     h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "feat_cols=[ 'year', 'mo', 'da', 'slp', 'stp', 'wdsp']\n",
    "# dataset=sqlcontext.sql(\"SELECT g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g\")\n",
    "dataset=df_all\n",
    "dataset=dataset.select(*(col(c).cast(\"double\").alias(c) for c in dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler=VectorAssembler(inputCols= feat_cols,outputCol='features')\n",
    "final_data=vec_assembler.transform(dataset)\n",
    "final_data=final_data.select(['features','rain_drizzle'])\n",
    "training_data,test_data=final_data.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|rain_drizzle|\n",
      "+--------------------+------------+\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Training Instances', 188385)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Training Instances\",training_data.filter(training_data.rain_drizzle==0).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Testing Instances', 62600)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Testing Instances\",test_data.filter(test_data.rain_drizzle==0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Training Instances', 49438)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Training Instances\",training_data.filter(training_data.rain_drizzle==1).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Testing Instances', 16570)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Testing Instances\",test_data.filter(test_data.rain_drizzle==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='rain_drizzle')\n",
    "model=lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.transform(training_data)\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.33150276090169195)\n",
      "('F1 Score-Training Data', 0.7002404333530079)\n",
      "('Accuracy Score-Training Data', 0.7921227131101701)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.32899966512814693)\n",
      "('F1 Score-Test Data', 0.698286551283027)\n",
      "('Accuracy Score-Test Data', 0.790703549324239)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol='features',labelCol='rain_drizzle', maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel=dt.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('numNodes = ', 15)\n",
      "('depth = ', 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dtModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtModel.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train=dtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.3268999400238934)\n",
      "('F1 Score-Training Data', 0.7670036603048671)\n",
      "('Accuracy Score-Training Data', 0.8154593962736993)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.3292165066910786)\n",
      "('F1 Score-Test Data', 0.764595002982875)\n",
      "('Accuracy Score-Test Data', 0.813452065176203)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='features',labelCol='rain_drizzle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(training_data)\n",
    "predictions = rfModel.transform(test_data)\n",
    "pred_train=dtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.3268999400238934)\n",
      "('F1 Score-Training Data', 0.7670036603048671)\n",
      "('Accuracy Score-Training Data', 0.8154593962736993)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.7212521166331598)\n",
      "('F1 Score-Test Data', 0.7893851535853862)\n",
      "('Accuracy Score-Test Data', 0.8318176076796766)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol='features',labelCol='rain_drizzle',maxIter=10)\n",
    "gbtModel = gbt.fit(training_data)\n",
    "predictions = gbtModel.transform(test_data)\n",
    "pred_train=dtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.3268999400238934)\n",
      "('F1 Score-Training Data', 0.7670036603048671)\n",
      "('Accuracy Score-Training Data', 0.8154593962736993)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.8490888483532845)\n",
      "('F1 Score-Test Data', 0.9070315890719735)\n",
      "('Accuracy Score-Test Data', 0.9131615510925856)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
