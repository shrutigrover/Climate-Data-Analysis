{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import timeit\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "spark = SparkSession.builder.appName(\"Project\").master(\"local[2]\").config('spark.ui.port', random.randrange(4000,5000)).getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPreProcessing(df):  \n",
    "    #casting year, month and day to integer values\n",
    "    df = df.withColumn(\"year\", df[\"year\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"mo\", df[\"mo\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"da\", df[\"da\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"temp\", df[\"temp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"slp\", df[\"slp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"stp\", df[\"stp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"wdsp\", df[\"wdsp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"mxpsd\", df[\"mxpsd\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"max\", df[\"max\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"min\", df[\"min\"].cast(FloatType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_and_set_views():\n",
    "    global stations, gsod_data_2009, gsod_data_2010, gsod_data_2011, gsod_data_2012, gsod_data_2013, gsod_data_2014, gsod_data_2015, gsod_data_2016, gsod_data_2017, gsod_data_2018, gsod_data_2019\n",
    "\n",
    "    gsod_data_2009 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2009.txt\").cache()\n",
    "#     gsod_data_2009=performPreProcessing(gsod_data_2009)\n",
    "    gsod_data_2009.createOrReplaceTempView(\"gsod_data_2009\")\n",
    "\n",
    "    gsod_data_2010 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2010.txt\").cache()\n",
    "#     gsod_data_2010 = performPreProcessing(gsod_data_2010)\n",
    "    gsod_data_2010.createOrReplaceTempView(\"gsod_data_2010\")\n",
    "\n",
    "    gsod_data_2011 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2011.txt\").cache()\n",
    "#     gsod_data_2011 =performPreProcessing(gsod_data_2011)\n",
    "    gsod_data_2011.createOrReplaceTempView(\"gsod_data_2011\")\n",
    "\n",
    "    gsod_data_2012 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2012.txt\").cache()\n",
    "#     gsod_data_2012 =performPreProcessing(gsod_data_2012)\n",
    "    gsod_data_2012.createOrReplaceTempView(\"gsod_data_2012\")\n",
    "\n",
    "    gsod_data_2013 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2013.txt\").cache()\n",
    "#     gsod_data_2013 =performPreProcessing(gsod_data_2013)\n",
    "    gsod_data_2013.createOrReplaceTempView(\"gsod_data_2013\")\n",
    "\n",
    "    gsod_data_2014 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2014.txt\").cache()\n",
    "#     gsod_data_2014 =performPreProcessing(gsod_data_2014)\n",
    "    gsod_data_2014.createOrReplaceTempView(\"gsod_data_2014\")\n",
    "\n",
    "    gsod_data_2015 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2015.txt\").cache()\n",
    "#     gsod_data_2015 =performPreProcessing(gsod_data_2015)\n",
    "    gsod_data_2015.createOrReplaceTempView(\"gsod_data_2015\")\n",
    "\n",
    "    gsod_data_2016 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2016.txt\").cache()\n",
    "#     gsod_data_2016 =performPreProcessing(gsod_data_2016)\n",
    "    gsod_data_2016.createOrReplaceTempView(\"gsod_data_2016\")\n",
    "\n",
    "    gsod_data_2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2017.txt\").cache()\n",
    "#     gsod_data_2017 =performPreProcessing(gsod_data_2017)\n",
    "    gsod_data_2017.createOrReplaceTempView(\"gsod_data_2017\")\n",
    "\n",
    "    gsod_data_2018 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2018.txt\").cache()\n",
    "#     gsod_data_2018 =performPreProcessing(gsod_data_2018)\n",
    "    gsod_data_2018.createOrReplaceTempView(\"gsod_data_2018\")\n",
    "\n",
    "    gsod_data_2019 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2019.txt\").cache()\n",
    "#     gsod_data_2019 =performPreProcessing(gsod_data_2019)\n",
    "    gsod_data_2019.createOrReplaceTempView(\"gsod_data_2019\")\n",
    "\n",
    "    stations = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/stations.txt\").cache()\n",
    "    stations.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "load_datasets_and_set_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lr_train=[]\n",
    "rmse_glr_train=[]\n",
    "rmse_lr_test=[]\n",
    "rmse_glr_test=[]\n",
    "for month in [\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]:\n",
    "\n",
    "    print(month,\".................\\n\")\n",
    "    # month=\"01\"\n",
    "    country=\"US\"\n",
    "\n",
    "    query_2009=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2009 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2010=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2010 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2011=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2011 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2012=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2012 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2013=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2013 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2014=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2014 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2015=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2015 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2016=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2016 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2017=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2017 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2018=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2019=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "    query_2009 =performPreProcessing(query_2009)\n",
    "    query_2010 =performPreProcessing(query_2010)\n",
    "    query_2011 =performPreProcessing(query_2011)\n",
    "    query_2012 =performPreProcessing(query_2012)\n",
    "    query_2013 =performPreProcessing(query_2013)\n",
    "    query_2014 =performPreProcessing(query_2014)\n",
    "    query_2015 =performPreProcessing(query_2015)\n",
    "    query_2016 =performPreProcessing(query_2016)\n",
    "    query_2017 =performPreProcessing(query_2017)\n",
    "    query_2018 =performPreProcessing(query_2018)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ### Table Union\n",
    "\n",
    "    df_all=query_2017.union(query_2018)\\\n",
    "#                     .union(query_2011)\\\n",
    "#                     .union(query_2012)\\\n",
    "#                     .union(query_2013)\\\n",
    "#                     .union(query_2014)\\\n",
    "#                     .union(query_2015)\\\n",
    "#                     .union(query_2016)\\\n",
    "#                     .union(query_2017)\\\n",
    "#                     .union(query_2018)\\\n",
    "#                     .union(query_2019)\n",
    "\n",
    "    \n",
    "    ##Monthly Average/Max/Min Temperature\n",
    "    df_min=df_all.groupBy('stn').agg({'temp':'min'})\n",
    "    df_max=df_all.groupBy('stn').agg({'temp':'max'})\n",
    "    df_avg=df_all.groupBy('stn').agg({'temp':'avg'})\n",
    "    # df_all.show(5)\n",
    "\n",
    "    ### Feature Selection for Regression\n",
    "    \n",
    "\n",
    "    from pyspark.sql.functions import col\n",
    "    feat_cols_reg=[ 'year', 'mo', 'da', 'slp', 'stp', 'wdsp','max','min']\n",
    "    # dataset=sqlcontext.sql(\"SELECT g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g\")\n",
    "    dataset_reg=df_all\n",
    "\n",
    "    dataset_reg=dataset_reg.select(*(col(c).cast(\"double\").alias(c) for c in dataset_reg.columns))\n",
    "\n",
    "\n",
    "    vec_assembler_reg=VectorAssembler(inputCols= feat_cols_reg,outputCol='features')\n",
    "    final_data_reg=vec_assembler_reg.transform(dataset_reg)\n",
    "    final_data_reg=final_data_reg.select(['features','temp'])\n",
    "    training_data_reg,test_data_reg=final_data_reg.randomSplit([0.75,0.25],seed=100)\n",
    "\n",
    "\n",
    "    ### Feature Selection for Regression\n",
    "\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "    start = timeit.timeit()\n",
    "\n",
    "    lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,labelCol='temp')\n",
    "\n",
    "    # Fit the model\n",
    "    model = lr.fit(training_data_reg)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(test_data_reg)\n",
    "    pred_train = model.transform(training_data_reg)\n",
    "    end = timeit.timeit()\n",
    "#     print(\"Execution time Linear Regression 2009-10\",end - start)\n",
    "    # Select example rows to display.\n",
    "#     predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    rmse_lr_test.append(rmse)\n",
    "    print(\"Test RMSE for {}-Linear Regression\".format(month), rmse)\n",
    "\n",
    "\n",
    "\n",
    "    ### Generalized Linear Regression\n",
    "\n",
    "    from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "    # Load training data\n",
    "    # dataset = spark.read.format(\"libsvm\")\\\n",
    "    #     .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "    start= timeit.timeit()\n",
    "    glr = GeneralizedLinearRegression(labelCol='temp',family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "    # Fit the model\n",
    "    model = glr.fit(training_data_reg)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(test_data_reg)\n",
    "    pred_train= model.transform(training_data_reg)\n",
    "    end=timeit.timeit()\n",
    "\n",
    "#     print(\"Execution time Generalized Linear Regression 2019-10\",end - start)\n",
    "\n",
    "    # Select example rows to display.\n",
    "#     predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    rmse_glr_test.append(rmse)\n",
    "    print(\"Test RMSE for {}-GLR\".format(month), rmse,\"\\n\\n\")\n",
    "\n",
    "print(\"Test rmse month wise Linear Regression\",rmse_lr_test)\n",
    "print(\"Test rmse month wise GLR\",rmse_glr_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('01', '.................\\n')\n",
      "('Test RMSE for 01-Linear Regression', 19.82529838241916)\n",
      "('Test RMSE for 01-GLR', 19.811872517036257, '\\n\\n')\n",
      "('02', '.................\\n')\n",
      "('Test RMSE for 02-Linear Regression', 19.32978177367141)\n",
      "('Test RMSE for 02-GLR', 19.31680842430419, '\\n\\n')\n",
      "('03', '.................\\n')\n",
      "('Test RMSE for 03-Linear Regression', 19.972827452569224)\n",
      "('Test RMSE for 03-GLR', 19.958497124987378, '\\n\\n')\n",
      "('04', '.................\\n')\n",
      "('Test RMSE for 04-Linear Regression', 15.653844602306176)\n",
      "('Test RMSE for 04-GLR', 15.646385229670612, '\\n\\n')\n",
      "('05', '.................\\n')\n",
      "('Test RMSE for 05-Linear Regression', 12.655812612895145)\n",
      "('Test RMSE for 05-GLR', 12.63234714619344, '\\n\\n')\n",
      "('06', '.................\\n')\n",
      "('Test RMSE for 06-Linear Regression', 11.3433329694773)\n",
      "('Test RMSE for 06-GLR', 11.31837189717347, '\\n\\n')\n",
      "('07', '.................\\n')\n",
      "('Test RMSE for 07-Linear Regression', 10.467897821983168)\n",
      "('Test RMSE for 07-GLR', 10.442470699551729, '\\n\\n')\n",
      "('08', '.................\\n')\n",
      "('Test RMSE for 08-Linear Regression', 11.189383844590386)\n",
      "('Test RMSE for 08-GLR', 11.164085253139017, '\\n\\n')\n",
      "('09', '.................\\n')\n",
      "('Test RMSE for 09-Linear Regression', 13.015044103817623)\n",
      "('Test RMSE for 09-GLR', 12.99199066795104, '\\n\\n')\n",
      "('10', '.................\\n')\n",
      "('Test RMSE for 10-Linear Regression', 15.276620900502927)\n",
      "('Test RMSE for 10-GLR', 15.268733663098764, '\\n\\n')\n",
      "('11', '.................\\n')\n",
      "('Test RMSE for 11-Linear Regression', 17.372782820305005)\n",
      "('Test RMSE for 11-GLR', 17.365775173209023, '\\n\\n')\n",
      "('12', '.................\\n')\n",
      "('Test RMSE for 12-Linear Regression', 17.317772199343285)\n",
      "('Test RMSE for 12-GLR', 17.311232675736854, '\\n\\n')\n",
      "('Test rmse month wise Linear Regression', [19.82529838241916, 19.32978177367141, 19.972827452569224, 15.653844602306176, 12.655812612895145, 11.3433329694773, 10.467897821983168, 11.189383844590386, 13.015044103817623, 15.276620900502927, 17.372782820305005, 17.317772199343285])\n",
      "('Test rmse month wise GLR', [19.811872517036257, 19.31680842430419, 19.958497124987378, 15.646385229670612, 12.63234714619344, 11.31837189717347, 10.442470699551729, 11.164085253139017, 12.99199066795104, 15.268733663098764, 17.365775173209023, 17.311232675736854])\n"
     ]
    }
   ],
   "source": [
    "rmse_lr_train=[]\n",
    "rmse_glr_train=[]\n",
    "rmse_lr_test=[]\n",
    "rmse_glr_test=[]\n",
    "for month in [\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]:\n",
    "\n",
    "    print(month,\".................\\n\")\n",
    "    # month=\"01\"\n",
    "    country=\"US\"\n",
    "\n",
    "    query_2009=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2009 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2010=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2010 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2011=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2011 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2012=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2012 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2013=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2013 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2014=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2014 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2015=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2015 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2016=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2016 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2017=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2017 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2018=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "    query_2019=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "    query_2009 =performPreProcessing(query_2009)\n",
    "    query_2010 =performPreProcessing(query_2010)\n",
    "    query_2011 =performPreProcessing(query_2011)\n",
    "    query_2012 =performPreProcessing(query_2012)\n",
    "    query_2013 =performPreProcessing(query_2013)\n",
    "    query_2014 =performPreProcessing(query_2014)\n",
    "    query_2015 =performPreProcessing(query_2015)\n",
    "    query_2016 =performPreProcessing(query_2016)\n",
    "    query_2017 =performPreProcessing(query_2017)\n",
    "    query_2018 =performPreProcessing(query_2018)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ### Table Union\n",
    "\n",
    "    df_all=query_2009.union(query_2010)\\\n",
    "                    .union(query_2011)\\\n",
    "                    .union(query_2012)\\\n",
    "                    .union(query_2013)\\\n",
    "                    .union(query_2014)\\\n",
    "                    .union(query_2015)\\\n",
    "                    .union(query_2016)\\\n",
    "                    .union(query_2017)\\\n",
    "                    .union(query_2018)\\\n",
    "                    .union(query_2019)\n",
    "\n",
    "    \n",
    "    ##Monthly Average/Max/Min Temperature\n",
    "    df_min=df_all.groupBy('stn').agg({'temp':'min'})\n",
    "    df_max=df_all.groupBy('stn').agg({'temp':'max'})\n",
    "    df_avg=df_all.groupBy('stn').agg({'temp':'avg'})\n",
    "    # df_all.show(5)\n",
    "\n",
    "    ### Feature Selection for Regression\n",
    "    \n",
    "\n",
    "    from pyspark.sql.functions import col\n",
    "    feat_cols_reg=[ 'year', 'mo', 'da', 'slp', 'stp', 'wdsp']\n",
    "    # dataset=sqlcontext.sql(\"SELECT g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g\")\n",
    "    dataset_reg=df_all\n",
    "\n",
    "    dataset_reg=dataset_reg.select(*(col(c).cast(\"double\").alias(c) for c in dataset_reg.columns))\n",
    "\n",
    "\n",
    "    vec_assembler_reg=VectorAssembler(inputCols= feat_cols_reg,outputCol='features')\n",
    "    final_data_reg=vec_assembler_reg.transform(dataset_reg)\n",
    "    final_data_reg=final_data_reg.select(['features','temp'])\n",
    "    training_data_reg,test_data_reg=final_data_reg.randomSplit([0.75,0.25],seed=100)\n",
    "\n",
    "\n",
    "    ### Feature Selection for Regression\n",
    "\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "    start = timeit.timeit()\n",
    "\n",
    "    lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,labelCol='temp')\n",
    "\n",
    "    # Fit the model\n",
    "    model = lr.fit(training_data_reg)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(test_data_reg)\n",
    "    pred_train = model.transform(training_data_reg)\n",
    "    end = timeit.timeit()\n",
    "#     print(\"Execution time Linear Regression 2009-10\",end - start)\n",
    "    # Select example rows to display.\n",
    "#     predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    rmse_lr_test.append(rmse)\n",
    "    print(\"Test RMSE for {}-Linear Regression\".format(month), rmse)\n",
    "\n",
    "\n",
    "\n",
    "    ### Generalized Linear Regression\n",
    "\n",
    "    from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "    # Load training data\n",
    "    # dataset = spark.read.format(\"libsvm\")\\\n",
    "    #     .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "    start= timeit.timeit()\n",
    "    glr = GeneralizedLinearRegression(labelCol='temp',family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "    # Fit the model\n",
    "    model = glr.fit(training_data_reg)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(test_data_reg)\n",
    "    pred_train= model.transform(training_data_reg)\n",
    "    end=timeit.timeit()\n",
    "\n",
    "#     print(\"Execution time Generalized Linear Regression 2019-10\",end - start)\n",
    "\n",
    "    # Select example rows to display.\n",
    "#     predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    rmse_glr_test.append(rmse)\n",
    "    print(\"Test RMSE for {}-GLR\".format(month), rmse,\"\\n\\n\")\n",
    "\n",
    "print(\"Test rmse month wise Linear Regression\",rmse_lr_test)\n",
    "print(\"Test rmse month wise GLR\",rmse_glr_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lr=[]\n",
    "rmse_glr=[]\n",
    "month=\"01\"\n",
    "country=\"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2009=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2009 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2010=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2010 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2011=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2011 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2012=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2012 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2013=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2013 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2014=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2014 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2015=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2015 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2016=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2016 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2017=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2017 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2018=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2019=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2009 =performPreProcessing(query_2009)\n",
    "query_2010 =performPreProcessing(query_2010)\n",
    "query_2011 =performPreProcessing(query_2011)\n",
    "query_2012 =performPreProcessing(query_2012)\n",
    "query_2013 =performPreProcessing(query_2013)\n",
    "query_2014 =performPreProcessing(query_2014)\n",
    "query_2015 =performPreProcessing(query_2015)\n",
    "query_2016 =performPreProcessing(query_2016)\n",
    "query_2017 =performPreProcessing(query_2017)\n",
    "query_2018 =performPreProcessing(query_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=query_2017.union(query_2018)\\\n",
    "#                 .union(query_2011)\\\n",
    "#                 .union(query_2012)\\\n",
    "#                 .union(query_2013)\\\n",
    "#                 .union(query_2014)\\\n",
    "#                 .union(query_2015)\\\n",
    "#                 .union(query_2016)\\\n",
    "#                 .union(query_2017)\\\n",
    "#                 .union(query_2018)\\\n",
    "#                 .union(query_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Monthly Average Temperature\n",
    "df_min=df_all.groupBy('stn').agg({'temp':'min'})\n",
    "df_max=df_all.groupBy('stn').agg({'temp':'max'})\n",
    "df_avg=df_all.groupBy('stn').agg({'temp':'avg'})\n",
    "# df_all.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "feat_cols_reg=[ 'year', 'mo', 'da', 'slp', 'stp', 'wdsp', 'mxpsd', 'max', 'min']\n",
    "# dataset=sqlcontext.sql(\"SELECT g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g\")\n",
    "dataset_reg=df_all\n",
    "\n",
    "dataset_reg=dataset_reg.select(*(col(c).cast(\"double\").alias(c) for c in dataset_reg.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler_reg=VectorAssembler(inputCols= feat_cols_reg,outputCol='features')\n",
    "final_data_reg=vec_assembler_reg.transform(dataset_reg)\n",
    "final_data_reg=final_data_reg.select(['features','temp'])\n",
    "training_data_reg,test_data_reg=final_data_reg.randomSplit([0.75,0.25],seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Execution time Linear Regression 2017-18', 0.010365962982177734)\n",
      "+------------------+------------------+--------------------+\n",
      "|        prediction|              temp|            features|\n",
      "+------------------+------------------+--------------------+\n",
      "|12.145587618247177|13.300000190734863|[2017.0,1.0,1.0,1...|\n",
      "|31.999954649711942|31.700000762939453|[2017.0,1.0,1.0,1...|\n",
      "| 36.67853119977634|              36.0|[2017.0,1.0,1.0,1...|\n",
      "| 36.67853119977634|              36.0|[2017.0,1.0,1.0,1...|\n",
      "| 33.61866391700639|33.900001525878906|[2017.0,1.0,1.0,1...|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "('Test RMSE for month-Linear Regression', 1.5868863764744965)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "start = timeit.timeit()\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,labelCol='temp')\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(training_data_reg)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data_reg)\n",
    "pred_train = model.transform(training_data_reg)\n",
    "end = timeit.timeit()\n",
    "print(\"Execution time Linear Regression 2017-18\",end - start)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Test RMSE for month-Linear Regression\".format(month), rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Execution time Generalized Linear Regression 2017-18', 0.0014338493347167969)\n",
      "+------------------+------------------+--------------------+\n",
      "|        prediction|              temp|            features|\n",
      "+------------------+------------------+--------------------+\n",
      "|11.838353256453757|13.300000190734863|[2017.0,1.0,1.0,1...|\n",
      "| 32.43066880285326|31.700000762939453|[2017.0,1.0,1.0,1...|\n",
      "| 36.71637400035709|              36.0|[2017.0,1.0,1.0,1...|\n",
      "| 36.71637400035709|              36.0|[2017.0,1.0,1.0,1...|\n",
      "|33.653294848642474|33.900001525878906|[2017.0,1.0,1.0,1...|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "('Test RMSE for month-Linear Regression', 1.5560629094967058)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load training data\n",
    "# dataset = spark.read.format(\"libsvm\")\\\n",
    "#     .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "start= timeit.timeit()\n",
    "glr = GeneralizedLinearRegression(labelCol='temp',family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(training_data_reg)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data_reg)\n",
    "pred_train= model.transform(training_data_reg)\n",
    "end=timeit.timeit()\n",
    "\n",
    "print(\"Execution time Generalized Linear Regression 2017-18\",end - start)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Test RMSE for month-Linear Regression\".format(month), rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
