{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import timeit\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "spark = SparkSession.builder.appName(\"Project\").master(\"local[*]\").config('spark.ui.port', random.randrange(4000,5000)).getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPreProcessing(df):  \n",
    "    #casting year, month and day to integer values\n",
    "    df = df.withColumn(\"year\", df[\"year\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"mo\", df[\"mo\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"da\", df[\"da\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"temp\", df[\"temp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"slp\", df[\"slp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"stp\", df[\"stp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"wdsp\", df[\"wdsp\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"mxpsd\", df[\"mxpsd\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"max\", df[\"max\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"min\", df[\"min\"].cast(FloatType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_and_set_views():\n",
    "    global stations, gsod_data_2009, gsod_data_2010, gsod_data_2011, gsod_data_2012, gsod_data_2013, gsod_data_2014, gsod_data_2015, gsod_data_2016, gsod_data_2017, gsod_data_2018, gsod_data_2019\n",
    "\n",
    "    gsod_data_2009 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2009.txt\").cache()\n",
    "#     gsod_data_2009=performPreProcessing(gsod_data_2009)\n",
    "    gsod_data_2009.createOrReplaceTempView(\"gsod_data_2009\")\n",
    "\n",
    "    gsod_data_2010 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2010.txt\").cache()\n",
    "#     gsod_data_2010 = performPreProcessing(gsod_data_2010)\n",
    "    gsod_data_2010.createOrReplaceTempView(\"gsod_data_2010\")\n",
    "\n",
    "    gsod_data_2011 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2011.txt\").cache()\n",
    "#     gsod_data_2011 =performPreProcessing(gsod_data_2011)\n",
    "    gsod_data_2011.createOrReplaceTempView(\"gsod_data_2011\")\n",
    "\n",
    "    gsod_data_2012 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2012.txt\").cache()\n",
    "#     gsod_data_2012 =performPreProcessing(gsod_data_2012)\n",
    "    gsod_data_2012.createOrReplaceTempView(\"gsod_data_2012\")\n",
    "\n",
    "    gsod_data_2013 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2013.txt\").cache()\n",
    "#     gsod_data_2013 =performPreProcessing(gsod_data_2013)\n",
    "    gsod_data_2013.createOrReplaceTempView(\"gsod_data_2013\")\n",
    "\n",
    "    gsod_data_2014 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2014.txt\").cache()\n",
    "#     gsod_data_2014 =performPreProcessing(gsod_data_2014)\n",
    "    gsod_data_2014.createOrReplaceTempView(\"gsod_data_2014\")\n",
    "\n",
    "    gsod_data_2015 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2015.txt\").cache()\n",
    "#     gsod_data_2015 =performPreProcessing(gsod_data_2015)\n",
    "    gsod_data_2015.createOrReplaceTempView(\"gsod_data_2015\")\n",
    "\n",
    "    gsod_data_2016 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2016.txt\").cache()\n",
    "#     gsod_data_2016 =performPreProcessing(gsod_data_2016)\n",
    "    gsod_data_2016.createOrReplaceTempView(\"gsod_data_2016\")\n",
    "\n",
    "    gsod_data_2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2017.txt\").cache()\n",
    "#     gsod_data_2017 =performPreProcessing(gsod_data_2017)\n",
    "    gsod_data_2017.createOrReplaceTempView(\"gsod_data_2017\")\n",
    "\n",
    "    gsod_data_2018 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2018.txt\").cache()\n",
    "#     gsod_data_2018 =performPreProcessing(gsod_data_2018)\n",
    "    gsod_data_2018.createOrReplaceTempView(\"gsod_data_2018\")\n",
    "\n",
    "    gsod_data_2019 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/gsod2019.txt\").cache()\n",
    "#     gsod_data_2019 =performPreProcessing(gsod_data_2019)\n",
    "    gsod_data_2019.createOrReplaceTempView(\"gsod_data_2019\")\n",
    "\n",
    "    stations = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/tmp/Project/stations.txt\").cache()\n",
    "    stations.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "load_datasets_and_set_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsod_data_2009.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsod_data_2009.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=\"01\"\n",
    "country=\"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2009=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2009 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2010=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2010 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2011=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2011 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2012=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2012 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2013=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2013 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2014=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2014 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2015=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2015 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2016=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2016 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2017=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2017 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2018=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))\n",
    "\n",
    "query_2019=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn AND g.mo={0} AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_2019_all=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2019 g, stations s where s.usaf=g.stn  AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_2018_all=spark.sql(\"SELECT s.country, g.stn, g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g, stations s where s.usaf=g.stn AND s.country='{1}' AND g.slp!=9999.9 AND g.stp!=9999.9 AND g.wdsp!=999.9 AND g.mxpsd!=999.9 AND g.max!=9999.9 AND g.min!=9999.9\".format(month,country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2009 =performPreProcessing(query_2009)\n",
    "query_2010 =performPreProcessing(query_2010)\n",
    "query_2011 =performPreProcessing(query_2011)\n",
    "query_2012 =performPreProcessing(query_2012)\n",
    "query_2013 =performPreProcessing(query_2013)\n",
    "query_2014 =performPreProcessing(query_2014)\n",
    "query_2015 =performPreProcessing(query_2015)\n",
    "query_2016 =performPreProcessing(query_2016)\n",
    "query_2017 =performPreProcessing(query_2017)\n",
    "query_2018 =performPreProcessing(query_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=query_2009.union(query_2010)\\\n",
    "#                 .union(query_2011)\\\n",
    "#                 .union(query_2012)\\\n",
    "#                 .union(query_2013)\\\n",
    "#                 .union(query_2014)\\\n",
    "#                 .union(query_2015)\\\n",
    "#                 .union(query_2016)\\\n",
    "#                 .union(query_2017)\\\n",
    "#                 .union(query_2018)\\\n",
    "#                 .union(query_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all=query_2019_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Monthly Average Temperature\n",
    "df_min=df_all.groupBy('stn').agg({'temp':'min'})\n",
    "df_max=df_all.groupBy('stn').agg({'temp':'max'})\n",
    "df_avg=df_all.groupBy('stn').agg({'temp':'avg'})\n",
    "# df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_features = [t[0] for t in df_all.dtypes if (t[1] == 'float')]\n",
    "# print(numeric_features)\n",
    "# df_all.select(numeric_features).describe().toPandas().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# numeric_data = df_all.select(numeric_features).toPandas()\n",
    "# axs = scatter_matrix(numeric_data, figsize=(8, 8))\n",
    "# n = len(numeric_data.columns)\n",
    "# for i in range(n):\n",
    "#     v = axs[i, 0]\n",
    "#     v.yaxis.label.set_rotation(0)\n",
    "#     v.yaxis.label.set_ha('right')\n",
    "#     v.set_yticks(())\n",
    "#     h = axs[n-1, i]\n",
    "#     h.xaxis.label.set_rotation(90)\n",
    "#     h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "feat_cols=[ 'year', 'mo', 'da', 'temp', 'slp', 'stp', 'wdsp']\n",
    "# dataset=sqlcontext.sql(\"SELECT g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g\")\n",
    "dataset=df_all\n",
    "dataset=dataset.select(*(col(c).cast(\"double\").alias(c) for c in dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler=VectorAssembler(inputCols= feat_cols,outputCol='features')\n",
    "final_data=vec_assembler.transform(dataset)\n",
    "final_data=final_data.select(['features','rain_drizzle'])\n",
    "training_data,test_data=final_data.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|rain_drizzle|\n",
      "+--------------------+------------+\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "|[2009.0,1.0,1.0,1...|         0.0|\n",
      "|[2009.0,1.0,1.0,3...|         0.0|\n",
      "|[2009.0,1.0,1.0,3...|         0.0|\n",
      "|[2009.0,1.0,1.0,3...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Training Instances', 1518259)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Training Instances\",training_data.filter(training_data.rain_drizzle==0).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Testing Instances', 506659)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Testing Instances\",test_data.filter(test_data.rain_drizzle==0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Training Instances', 1119159)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Training Instances\",training_data.filter(training_data.rain_drizzle==1).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Testing Instances', 373022)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Testing Instances\",test_data.filter(test_data.rain_drizzle==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "feat_cols_reg=[ 'year', 'mo', 'da', 'slp', 'stp', 'wdsp']\n",
    "# dataset=sqlcontext.sql(\"SELECT g.year, g.mo, g.da, g.temp, g.slp, g.stp, g.wdsp, g.mxpsd, g.max, g.min, g.rain_drizzle FROM gsod_data_2018 g\")\n",
    "dataset_reg=df_all\n",
    "\n",
    "dataset_reg=dataset_reg.select(*(col(c).cast(\"double\").alias(c) for c in dataset_reg.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler_reg=VectorAssembler(inputCols= feat_cols,outputCol='features')\n",
    "final_data_reg=vec_assembler_reg.transform(dataset_reg)\n",
    "final_data_reg=final_data_reg.select(['features','temp'])\n",
    "training_data_reg,test_data_reg=final_data_reg.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "start = timeit.timeit()\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,labelCol='temp')\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(training_data_reg)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data_reg)\n",
    "pred_train = model.transform(training_data_reg)\n",
    "end = timeit.timeit()\n",
    "print(\"Execution time Linear Regression 2018-all months\",end - start)\n",
    "# Select example rows to display.\n",
    "# predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Test RMSE for month-Linear Regression\".format(month), rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load training data\n",
    "# dataset = spark.read.format(\"libsvm\")\\\n",
    "#     .load(\"data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "start= timeit.timeit()\n",
    "glr = GeneralizedLinearRegression(labelCol='temp',family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(training_data_reg)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data_reg)\n",
    "pred_train= model.transform(training_data_reg)\n",
    "end=timeit.timeit()\n",
    "\n",
    "print(\"Execution time Generalized Linear Regression 2017-18\",end - start)\n",
    "\n",
    "# Select example rows to display.\n",
    "# predictions.select(\"prediction\", \"temp\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Test RMSE for month-Linear Regression\".format(month), rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='rain_drizzle')\n",
    "model=lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.transform(training_data)\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.45987617674059217)\n",
      "('F1 Score-Training Data', 0.7001300037711943)\n",
      "('Accuracy Score-Training Data', 0.792042531875755)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.45842141707885714)\n",
      "('F1 Score-Test Data', 0.6986230133878076)\n",
      "('Accuracy Score-Test Data', 0.7909480045322926)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify layers for the neural network:\n",
    "# input layer of size 10 (features), two intermediate of size 5 and 4\n",
    "# and output of size 2 (classes)\n",
    "layers = [10, 100, 100, 50, 50, 10, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the trainer and set its parameters\n",
    "# trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, labelCol='rain_drizzle')\n",
    "\n",
    "# # train the model\n",
    "# model = trainer.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.transform(training_data)\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.transform(training_data)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.017325401753425094,0.0,-3.2822038619107886e-07,-1.2534441648861028e-07,-9.141572535689065e-06,-6.235416647438533e-07,-3.8455850414098013e-07]\n",
      "Intercept: -4.03394797045e-06\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# # Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1,labelCol='rain_drizzle')\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training_data)\n",
    "\n",
    "# Print the coefficients and intercept for linear SVC\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.21499328911665969)\n",
      "('F1 Score-Training Data', 0.7006044417924016)\n",
      "('Accuracy Score-Training Data', 0.7923869911188561)\n"
     ]
    }
   ],
   "source": [
    "pred_train = lsvcModel.transform(training_data)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# # Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.21991710736306064)\n",
      "('F1 Score-Test Data', 0.6971802451007683)\n",
      "('Accuracy Score-Test Data', 0.7898995535714286)\n"
     ]
    }
   ],
   "source": [
    "predictions = lsvcModel.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# # Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol='features',labelCol='rain_drizzle', maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel=dt.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('numNodes = ', 15)\n",
      "('depth = ', 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dtModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtModel.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train=dtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.6445209763215405)\n",
      "('F1 Score-Training Data', 0.7011381099393841)\n",
      "('Accuracy Score-Training Data', 0.708417474969838)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.6450109787756491)\n",
      "('F1 Score-Test Data', 0.7017896591280317)\n",
      "('Accuracy Score-Test Data', 0.7090058782672355)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='features',labelCol='rain_drizzle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(training_data)\n",
    "predictions = rfModel.transform(test_data)\n",
    "pred_train=dtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.6445209763215405)\n",
      "('F1 Score-Training Data', 0.7011381099393841)\n",
      "('Accuracy Score-Training Data', 0.708417474969838)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.7606480290788995)\n",
      "('F1 Score-Test Data', 0.7296950818502341)\n",
      "('Accuracy Score-Test Data', 0.731248031957039)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c7bf208dd69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgbtModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbtModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpred_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dtModel' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol='features',labelCol='rain_drizzle',maxIter=10)\n",
    "gbtModel = gbt.fit(training_data)\n",
    "predictions = gbtModel.transform(test_data)\n",
    "pred_train=gbtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train=gbtModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.8764420965566284)\n",
      "('F1 Score-Training Data', 0.9294537344915798)\n",
      "('Accuracy Score-Training Data', 0.9316802788217262)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Test Data', 0.876445695025784)\n",
      "('F1 Score-Test Data', 0.9291589439936595)\n",
      "('Accuracy Score-Test Data', 0.9313108766233766)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "# Evaluate model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle')\n",
    "# print(\"Area under ROC Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "dt = DecisionTreeClassifier(featuresCol='features',labelCol='rain_drizzle', maxDepth=3)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [10])\n",
    "             .addGrid(dt.maxBins, [40])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=dt,estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(training_data)\n",
    "# Takes ~5 minutes\n",
    "\n",
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test_data)\n",
    "pred_train=cvModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.7583943115513534)\n",
      "('F1 Score-Training Data', 0.8251767660047922)\n",
      "('Accuracy Score-Training Data', 0.8250103343787806)\n",
      "('Area under PR Curve-Test Data', 0.7588008963636595)\n",
      "('F1 Score-Test Data', 0.8252803400168938)\n",
      "('Accuracy Score-Test Data', 0.825104598707895)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model for Feb Data\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('numNodes = ', 1695)\n",
      "('depth = ', 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", cvModel.bestModel.numNodes)\n",
    "print(\"depth = \", cvModel.bestModel.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "gbt = GBTClassifier(featuresCol='features',labelCol='rain_drizzle',maxIter=10)\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [5,10,15])\n",
    "             .addGrid(gbt.maxBins, [40,60,80])\n",
    "             .addGrid(gbt.maxIter, [5,10,20])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=gbt,estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(training_data)\n",
    "# Takes ~5 minutes\n",
    "\n",
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test_data)\n",
    "pred_train=cvModel.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.9784062495646901)\n",
      "('F1 Score-Training Data', 0.9330291663565214)\n",
      "('Accuracy Score-Training Data', 0.9331909906971629)\n",
      "('Area under PR Curve-Test Data', 0.9774418605495953)\n",
      "('F1 Score-Test Data', 0.9315831930045926)\n",
      "('Accuracy Score-Test Data', 0.931752680146637)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model for Feb Data\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Area under PR Curve-Training Data', 0.5879496806756855)\n",
      "('F1 Score-Training Data', 0.6395215649337544)\n",
      "('Accuracy Score-Training Data', 0.6448777781415616)\n",
      "('Area under PR Curve-Test Data', 0.5876428140209834)\n",
      "('F1 Score-Test Data', 0.6397212786863695)\n",
      "('Accuracy Score-Test Data', 0.6450881078540227)\n"
     ]
    }
   ],
   "source": [
    "## 2019 All months data\n",
    "predictions = cvModel.transform(test_data)\n",
    "pred_train=cvModel.transform(training_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Training Data\",evaluator.evaluate(pred_train))\n",
    "\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Training Data\",evaluator2.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Training Data\",evaluator3.evaluate(pred_train))\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='rain_drizzle',metricName=\"areaUnderPR\")\n",
    "print(\"Area under PR Curve-Test Data\",evaluator.evaluate(predictions))\n",
    "evaluator2=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"f1\")\n",
    "print(\"F1 Score-Test Data\",evaluator2.evaluate(predictions))\n",
    "evaluator3=MulticlassClassificationEvaluator(labelCol='rain_drizzle',metricName=\"accuracy\")\n",
    "print(\"Accuracy Score-Test Data\",evaluator3.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
